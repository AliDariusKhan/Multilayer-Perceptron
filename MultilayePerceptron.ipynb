{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a multilayer perceptron whose output layer will be a one-hot encoded vector, so that we can use it for classification. It will automatically determine the size of the input and output layers, based on the data passed to it. You can also specify the hidden layer structure by passing it an array of integers, where the $i^{th}$ element is the number of nodes in the $i^{th}$ hidden layer (including the bias node). By default, it will generate one hidden layer and the number of nodes will be the average of the input and the output layer. We will implement stochastic gradient descent, mostly for the sake of code simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multilayer perceptron with $L$ layers, the $l^{th}$ layer is represented as $a_i^{l}$, a vector of activations. $a^{0}_i$ is one instance of the input data, and the corresponding output is the target vector $y_i$. In order to allow for constant shifts, we have that $a_l^0 =1$ for all $l \\neq L$. The $L^{th}$ layer is the output generated by the multilayer perceptron, therefore $a_i^L$ has the same dimension as $y_i$. For all layers where $l \\neq 0 $ and $l \\neq L $, the dimension of $a_i^l$ is a hyperparameter of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer $a_i^l$ (except when $l=L$) is associated with a weight matrix $\\Theta^l_{ij}$. The subsequent layer is generated by an activation function $\\sigma$ applied to $z_i = \\Theta^l_{ij}a_j^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a_i^{l+1}=\\sigma_i\\left(z_i^{l}\\right) = \\sigma_i\\left(\\Theta_{ij}^{l}a^{l}_j\\right) \\label{x} \\hspace{1cm} (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement applies for $i \\neq 0$ because this is the bias node, as explained above. The exception to this is when $l=L$, where it applies for all $i$ (including $i=0$) because there is no bias node in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of this model is a function of the model is some function of the output layer and the target variable: $J\\left(a^L_i,t_i \\right)$. We wish to minimize this with respect to the weight matrices $\\Theta^l_{ij}$. We will accomplish this by differentiating $J$ with respect to all the $\\Theta^l$, and performing gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\Theta^{l}_{ij}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the derivative of the cost function with respect to a weight matrix, $\\Theta^{l-1}_{ij}$, we we use the chain rule to decompose this into the derivative of the cost function with respect to the output layer, the derivates of an activation with respect to the previous activation and the derivative of the output layer with respect to the final matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\Theta^{l-1}_{ij}} = \\frac{\\partial J}{\\partial a^L_k} \\frac{\\partial a_k^{L}}{\\partial a_m^{L-1}}\\cdots \\frac{\\partial a_p^{l+1}}{\\partial a_q^l}\\frac{\\partial a^l_q}{\\partial \\Theta^{l-1}_{rs}} \\hspace{1cm} (2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use equation (1) to calculate the last derivative: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial a_m^{l+1}}{\\partial \\Theta^l_{ij}}&= \\frac{\\partial \\sigma_m\\left(z_m^{l+1}\\right)}{\\partial \\Theta^l_{ij}} \\\\\n",
    "&= \\sigma'_m\\left(z_m^{l+1}\\right)\\frac{\\partial z_m^{l+1}}{\\partial \\Theta^l_{ij}} \\\\\n",
    "&= \\sigma'_m\\left(z_m^{l+1}\\right)\\frac{\\partial \\left(\\Theta^l_{mk} a^l_k\\right)}{\\partial \\Theta^l_{ij}} \\\\\n",
    "&= \\sigma'_m\\left(z_m^{l+1}\\right)\\delta_{mi} a^l_j\n",
    "\\end{align}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kronecker delta in this expression indicates that the $m^{th}$ activation in layer $l+1$ only varies with respect to the parameters in the $m^{th}$ row of $\\Theta^l$. This is exactly what we expect, because the $m^{th}$ row expresses the strengths of the connections between all nodes in $a^{l-1}_i$ and the particular node $a^l_m$. If we use bold font for matrices and underlining for vectors, the above result be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial \\underline{a}^{l+1}}{\\partial \\bf{\\Theta}^l}\n",
    "&= \\underline{\\sigma}'\\left(\\underline{z}^{l+1}\\right) \\otimes\\underline{a}^l\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^l_0$ and $l\\neq L$ corresponds to the bias nodes, so the equation is instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial a_0^{l+1}}{\\partial \\Theta^l_{ij}}&= 0\n",
    "\\end{align}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of an activation layer with respect to the previous layer's activations is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial a_i^{l+1}}{\\partial a_j^l} &= \\frac{\\partial \\sigma_i\\left(z_i^{l+1}\\right)}{\\partial a_j^l} \\\\\n",
    "&= \\sigma_i'\\left(z_i^{l+1}\\right)\\frac{\\partial z_i^{l+1}}{\\partial a_j^l} \\\\\n",
    "&= \\sigma_i'\\left(z_i^{l+1}\\right)\\frac{\\partial \\left(\\Theta^l_{ik} a^l_k\\right)}{\\partial a_j^l} \\\\\n",
    "&= \\sigma_i'\\left(z_i^{l+1}\\right)\\Theta^l_{ij}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector notation, this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\underline{a}_i^{l+1}}{\\partial \\underline{a}_j^l}= \\underline{\\sigma}'\\left(\\underline{z}^{l+1}\\right)\\bf{\\Theta}^l $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, equation (2) can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\Theta^{l-1}_{ij}} = \\frac{\\partial J}{\\partial a^L_k}\\sigma_k'\\left(z_k^{L}\\right)\\Theta^{L-1}_{km}\\cdots \\sigma_p'\\left(z_p^{l+1}\\right)\\Theta^l_{pq}\\sigma'_q\\left(z_q^{l}\\right)\\delta_{qr} a^{l-1}_s\\hspace{1cm} (3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\mathbf{\\Theta}^{l-1}} = \\frac{\\partial J}{\\partial \\underline{a}^L} *\\underline{\\sigma}'\\left(\\underline{z}^{L}\\right)\\mathbf{\\Theta}^{L-1} *\\cdots *\\underline{\\sigma}'\\left(\\underline{z}^{l+1}\\right)\\mathbf{\\Theta}^{l}\\underline{\\sigma}'\\left(\\underline{z}^{l}\\right)\\otimes \\underline{a}^{l-1} \\hspace{1cm} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $*$ denotes element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification model, $t_i$ will be a one-hot encoded vector, so we want to convert $a^L$ into a one hot encoded vector as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-class classification model should have a hypothesis that corresponds to the multinomial distribution. In the framework of Generalized Linear Models, it can be shown that this is given by the response function:\n",
    "$$ s_i = \\frac{e^{a^L_i}}{\\sum_k e^{a^L_k}}$$\n",
    "This is known as the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function, found by maximizing the log likelihood function for the multinomial distribution, is:\n",
    "$$ J = -\\sum_i y_i \\ln s_i $$\n",
    "This is known as the cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete equation (3), we need to calculate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J}{\\partial a_k^{L}} =  -\\sum_i \\frac{y_i}{s_i} \\frac{\\partial s_j}{\\partial a_k^{L}} \\hspace{1cm} (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we need to find the derivative of the softmax function with respect to $a^L$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial s_i}{\\partial a^L_j} &= \\frac{\\delta_j^ie^{a^L_i}}{\\sum_k e^{a^L_k}}-\\frac{e^{a^L_i}}{\\left(\\sum_k e^{a^L_k}\\right)^2} \\frac{\\partial \\left(\\sum_k e^{a^L_k}\\right)}{\\partial a^L_j} \\\\\n",
    "&= \\frac{\\delta_j^i e^{a^L_i}}{\\sum_k e^{a^L_k}}-\\frac{e^{a^L_i}e^{a^L_j}}{\\left(\\sum_k e^{a^L_k}\\right)^2}  \\\\\n",
    "&= \\frac{e^{a^L_i}}{\\sum_k e^{a^L_k}}\\left(\\delta_j^i -\\frac{e^{a^L_j}}{\\left(\\sum_k e^{a^L_k}\\right)^2}  \\right) \\\\\n",
    "&= s_i\\left(\\delta_j^i -s_j  \\right) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this into equation (4):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial a^L_j} &= -\\sum_i y_i \\left(\\delta_j^i -s_j  \\right) \\\\\n",
    "&= -y_j(1-s_j) +\\sum_{i\\neq j} y_i s_j   \\\\\n",
    "&= -y_j yt_j s_j +\\sum_{i\\neq j} y_i s_j   \\\\\n",
    "&= -t_j +y_j \\sum_{i} t_i    \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $t_i$ is a one hot encoded vector, we have that $\\sum_{i} y_i = 1$. Therefore:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial a^L_j} &= s_j-y_j  \\\\\n",
    "&= \\frac{e^{a^L_j}}{\\sum_k e^{a^L_k}}-y_j \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, equation (3) for a multi-class classification model is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\Theta^{l-1}_{ij}} =\\left( \\frac{e^{a^L_j}}{\\sum_k e^{a^L_k}}-y_j  \\right)\\sigma_k'\\left(z_k^{L}\\right)\\Theta^{L-1}_{km}\\cdots \\sigma_p'\\left(z_p^{l+1}\\right)\\Theta^l_{pq}\\sigma'_q\\left(z_q^{l}\\right)\\delta_{qi} a^{l-1}_j\\hspace{1cm} (5)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial \\mathbf{\\Theta}^{l-1}} = \\left( \\frac{e^{\\underline{a}^L}}{\\sum_k e^{a^L_k}}-\\underline{y}  \\right) *\\underline{\\sigma}'\\left(\\underline{z}^{L}\\right)\\mathbf{\\Theta}^{L-1} * \\cdots * \\underline{\\sigma}'\\left(\\underline{z}^{l+1}\\right)\\mathbf{\\Theta}^{l}*\\underline{\\sigma}'\\left(\\underline{z}^{l}\\right)\\otimes \\underline{a}^{l-1} \\hspace{1cm} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Rectified Linear Unit, `ReLU`, as our activation function. The derivative of this function is the Heaviside step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weight_matrices = []\n",
    "        \n",
    "    def train(self, X, y, step_length, hidden_layer_sizes=None, iterations=1, initializations=None, scale=1):         \n",
    "        layer_sizes = [np.ma.size(X, 1), np.ma.size(y, 1)]\n",
    "        if not hidden_layer_sizes:\n",
    "            layer_sizes.insert(1, int(np.mean(layer_sizes)))\n",
    "        else:\n",
    "            layer_sizes[1:1] = hidden_layer_sizes\n",
    "        \n",
    "        if not initializations:\n",
    "            for l in range(len(layer_sizes)-1):\n",
    "                self.weight_matrices.append(scale* (np.random.rand(layer_sizes[l+1], layer_sizes[l])) )\n",
    "        else:\n",
    "            self.weight_matrices = initializations\n",
    "        \n",
    "        cost = []\n",
    "        for _ in range(iterations):\n",
    "            updated_weights = [np.copy(W) for W in self.weight_matrices]\n",
    "        \n",
    "            for i in range(len(X)):\n",
    "                activations, z = self.feedforward(X[i])\n",
    "               \n",
    "                if i == 0: cost.append(self.cost_function(y[i], activations[-1]))\n",
    "                delta = self.backpropagate(y[i], activations, z)\n",
    "                for d in range(len(delta)):\n",
    "                    updated_weights[d] -= step_length*delta[d]/len(X)\n",
    "                    \n",
    "            self.weight_matrices = [np.copy(w) for w in updated_weights]                \n",
    "            plt.plot(cost)\n",
    "        \n",
    "    def feedforward(self, X):\n",
    "        activations = [X]\n",
    "        z = activations.copy()\n",
    "        \n",
    "        for l in range(len(self.weight_matrices)):\n",
    "            z.append(self.weight_matrices[l].dot(activations[l]))\n",
    "            if l < len(self.weight_matrices) - 1:\n",
    "                z[l+1][0] = 1\n",
    "            activations.append(self.ReLU_transform(z[l+1]))            \n",
    "            \n",
    "        return activations, z\n",
    "    \n",
    "    def backpropagate(self, y, activations, z):\n",
    "        backpropagate_error = self.cost_function_derivative(activations[-1], y)\n",
    "        delta = []\n",
    "        for l in range(-1, -1-len(self.weight_matrices), -1):\n",
    "            delta.append(np.outer(np.multiply(backpropagate_error, self.heaviside(z[l])), activations[l-1]))\n",
    "            backpropagate_weight = np.copy(self.weight_matrices[l])\n",
    "            backpropagate_z = np.copy(z[l])\n",
    "            if l < -1:\n",
    "                backpropagate_weight = np.delete(backpropagate_weight, obj=0, axis=0)\n",
    "                backpropagate_error = np.delete(backpropagate_error, 0)\n",
    "                backpropagate_z = backpropagate_z[1:] \n",
    "            backpropagate_error = (np.multiply(backpropagate_error, self.heaviside(backpropagate_z))).dot(backpropagate_weight)\n",
    "            \n",
    "        return delta[::-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU_transform(input_feed):\n",
    "        return np.maximum(0, input_feed)\n",
    "    \n",
    "    @staticmethod\n",
    "    def heaviside(z):\n",
    "        return np.heaviside(z, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(activation):\n",
    "        return np.exp(activation-np.max(activation))/np.exp(activation-np.max(activation)).sum()    \n",
    "    \n",
    "    def cost_function(self, y, a):\n",
    "        a = a - a.max()\n",
    "        return -((np.multiply(y, a - (np.exp(a)).sum())).sum())\n",
    "    \n",
    "    def cost_function_derivative(self, activation, y):\n",
    "        return self.softmax(activation) - y\n",
    "    \n",
    "    def log_loss(self, X, y):\n",
    "        activations = np.array([])\n",
    "        for i in range(len(X)):\n",
    "            a, z = self.feedforward(X[i])\n",
    "            activations.append(a[-1])\n",
    "        return self.cost_function(y, activations)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for i in range(len(X)):\n",
    "            a, z = self.feedforward(X[i])\n",
    "            s = self.softmax(a[-1])\n",
    "            results.append(str(s.max())[:5] + \" probability that this is a \" +  str(np.argmax(s)))\n",
    "        return results\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
